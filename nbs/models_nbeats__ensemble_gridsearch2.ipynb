{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hhSHF8VQ7pa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# os.chdir('./drive/MyDrive/nixtlats')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GKQ8Id7l7pbD",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nixtlats/nbs\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.chdir('..')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rz243dgQ7pbH",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning\n",
    "# !pip install torchinfo\n",
    "# !pip install fastcore\n",
    "# !pip install s3fs\n",
    "# !pip install patool\n",
    "# !pip install --upgrade pandas==1.2.4\n",
    "# !pip install --upgrade requests==2.25.1\n",
    "# !pip install --upgrade scikit-learn==0.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5AtHGm2o7pbJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models.nbeats.tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zcdXcC-A7pbL",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gb71bFcG7pbM",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Callable, Dict, Iterable, Union, List\n",
    "from tqdm import tqdm\n",
    "import pylab as plt\n",
    "from pylab import rcParams\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from nixtlats.models.nbeats.nbeats import NBEATS\n",
    "from nixtlats.data.datasets.tourism import TourismInfo, Tourism\n",
    "from nixtlats.data.datasets.m4 import M4Info, M4, M4Evaluation\n",
    "from nixtlats.data.tsdataset import WindowsDataset\n",
    "from nixtlats.data.tsloader import TimeSeriesLoader\n",
    "from nixtlats.experiments.utils import create_datasets, get_mask_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e_UaJmVF7pbQ",
   "metadata": {},
   "source": [
    "# NBEATS Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SI9xKeQd7pbU",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class NBEATSHyperparameters:\n",
    "\n",
    "    dataset: str\n",
    "    frequency: str\n",
    "    group: type = field(init=False)\n",
    "    freq_grid: dict = field(init=False)\n",
    "    ensemble_grid: dict\n",
    "    grid: dict = field(init=False)\n",
    "\n",
    "    # Common grid to all frequencies\n",
    "    common_grid = {}\n",
    "\n",
    "    # Architecture parameters\n",
    "    common_grid['activation'] = ['ReLU'] # Oreshkin\n",
    "    common_grid['n_x'] = [0] # No exogenous variables\n",
    "    common_grid['n_s'] = [0] # No static variables\n",
    "    common_grid['n_x_hidden'] = [0] # No exogenous variables\n",
    "    common_grid['n_s_hidden'] = [0] # No static variables\n",
    "    common_grid['stack_types'] = [['trend', 'seasonality']] # NBEATS-I original architecture\n",
    "    common_grid['n_blocks'] = [[3, 3]] # Trend blocks, Seasonal blocks - Oreshkin\n",
    "    common_grid['n_layers'] = [[4, 4]] # Trend-block layers, Seasonal-block - Oreshkin\n",
    "    common_grid['shared_weights'] = [True] # Oreshkin\n",
    "    common_grid['n_harmonics'] = [1] # Oreshkin\n",
    "    common_grid['n_polynomials'] = [2] # Trend polynomial degree\n",
    "    common_grid['n_theta_hidden'] = [[common_grid['n_layers'][0][0] * [256],\n",
    "                                    common_grid['n_layers'][0][1] * [2048]]] # Oreshkin\n",
    "    common_grid['initialization'] = ['lecun_normal'] # Arbitrary\n",
    "\n",
    "    # Optimization parameters\n",
    "    # common_grid['learning_rate'] = [0.0001] # Oreshkin\n",
    "    common_grid['lr_decay'] = [0] # No lr_decay in the original implementation\n",
    "    common_grid['lr_decay_step_size'] = [1_000] # No lr_decay in the original implementation\n",
    "    common_grid['dropout_prob_theta'] = [0] # No dropout in the original implementation\n",
    "    common_grid['weight_decay'] = [0] # # No weight_decay in the original implementation\n",
    "    common_grid['batch_size'] = [1024] # Oreshkin\n",
    "    common_grid['batch_normalization'] = [False] # No batch_normalization in the original implementation\n",
    "    common_grid['train_sample_freq'] = [1] \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \n",
    "        \n",
    "        assert self.dataset in ['M4', 'Tourism'], \\\n",
    "            f\"Dataset must be one of the following: {['M4', 'Tourism']}\"\n",
    "        \n",
    "        if self.dataset == 'M4': \n",
    "            assert self.frequency in ['Yearly', 'Quarterly', 'Monthly', 'Weekly', 'Daily', 'Hourly'], \\\n",
    "                f\"Frequency must be one of the following: {['Yearly', 'Quarterly', 'Monthly', 'Weekly', 'Daily', 'Hourly']}\"\n",
    "            self.group = M4Info[self.frequency]\n",
    "            self.common_grid['l_h'] = [10 if self.frequency in ['Weekly', 'Daily', 'Hourly'] else 1.5]\n",
    "            self.common_grid['loss_val'] = ['SMAPE']\n",
    "        if self.dataset == 'Tourism': \n",
    "            assert self.frequency in ['Yearly', 'Quarterly', 'Monthly'], \\\n",
    "                f\"Frequency must be one of the following: {['Yearly', 'Quarterly', 'Monthly']}\"\n",
    "            self.group = TourismInfo[self.frequency]\n",
    "            self.common_grid['l_h'] = [10 if self.frequency in ['Quarterly'] else 20]\n",
    "            self.common_grid['loss_val'] = ['MAPE']\n",
    "        \n",
    "        # Frequency grid\n",
    "        self.freq_grid = {}\n",
    "        self.freq_grid['n_time_in'] = [self.group.horizon * i for i in self.ensemble_grid['lookbacks']]\n",
    "        self.freq_grid['n_time_out'] = [self.group.horizon]\n",
    "        self.freq_grid['frequency'] = [self.group.freq]\n",
    "        self.freq_grid['seasonality'] = [self.group.seasonality]\n",
    "\n",
    "        # Ensemble grid\n",
    "        self.ensemble_grid = self.ensemble_grid.copy()\n",
    "        self.ensemble_grid.pop('lookbacks')\n",
    "\n",
    "        # Full hyperparameter grid\n",
    "        self.grid = {**self.common_grid,\n",
    "                     **self.freq_grid}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UUCRoN437pba",
   "metadata": {},
   "source": [
    "# Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TvSy3Fuy7pbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def complete_series(Y_df, hparams):\n",
    "    Y_complete_df = []\n",
    "    series_counts = Y_df.iloc[:,:2].groupby(['unique_id']).agg('count').reset_index()\n",
    "    series_counts.rename(columns={series_counts.columns[1]: 'n'}, inplace=True)\n",
    "    n_in_out = hparams['n_time_in'] + hparams['n_time_out']\n",
    "\n",
    "    for serie_id in tqdm(Y_df.unique_id.unique(), position=0, leave=True):\n",
    "        serie_count = series_counts[series_counts.unique_id == serie_id].n.iloc[0]\n",
    "        Y_df_id = Y_df[Y_df.unique_id == serie_id]\n",
    "        if serie_count < n_in_out:\n",
    "            reps = n_in_out - serie_count\n",
    "            Y_complete_df.append(pd.concat([Y_df_id.iloc[[0],:]] * reps + [Y_df_id], axis=0))\n",
    "        else:\n",
    "            Y_complete_df.append(Y_df_id)\n",
    "\n",
    "    Y_complete_df = pd.concat(Y_complete_df, axis=0).reset_index(drop=True)\n",
    "\n",
    "    return Y_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FQqEKxsY7pbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_loaders(Y_df, S_df, dataset, hparams, num_workers):\n",
    "\n",
    "    print(f'Instantiating loaders (n_time_in = {hparams[\"n_time_in\"]})...')\n",
    "\n",
    "    # To prevent series shorter than n_time_in + n_time_out\n",
    "    if dataset == 'Tourism': series_df = complete_series(Y_df, hparams)\n",
    "    else: series_df = Y_df\n",
    "    \n",
    "    train_mask_df, val_mask_df, _ = get_mask_dfs(Y_df=series_df,\n",
    "                                                        ds_in_test=0,\n",
    "                                                        ds_in_val=hparams['n_time_out'])\n",
    "\n",
    "    train_dataset = WindowsDataset(Y_df=series_df, S_df=S_df,\n",
    "                                   mask_df=train_mask_df,\n",
    "                                   input_size=hparams['n_time_in'],\n",
    "                                   output_size=hparams['n_time_out'],\n",
    "                                   sample_freq=hparams['train_sample_freq'],\n",
    "                                   complete_sample=False)\n",
    "\n",
    "    test_dataset = WindowsDataset(Y_df=series_df, S_df=S_df,\n",
    "                                    mask_df=val_mask_df,\n",
    "                                    input_size=hparams['n_time_in'],\n",
    "                                    output_size=hparams['n_time_out'],\n",
    "                                    sample_freq=hparams['train_sample_freq'],\n",
    "                                    complete_sample=True)\n",
    "    \n",
    "    train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                    batch_size=int(hparams['batch_size']),\n",
    "                                    eq_batch_size=True,\n",
    "                                    num_workers=4,\n",
    "                                    shuffle=True)\n",
    "    \n",
    "    val_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                    batch_size=int(hparams['batch_size']),\n",
    "                                    eq_batch_size=True,\n",
    "                                    num_workers=4,\n",
    "                                    shuffle=False)\n",
    "        \n",
    "    test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                        batch_size=int(hparams['batch_size']),\n",
    "                                        eq_batch_size=False,\n",
    "                                        num_workers=4,\n",
    "                                        shuffle=False)\n",
    "    \n",
    "    print('\\nData loaders ready.\\n')\n",
    "    \n",
    "    del train_dataset, test_dataset\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KVja5UUD7pbh",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wnkYKSja7pbi",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _parameter_grid(grid):\n",
    "    specs_list = list(product(*list(grid.values())))\n",
    "    model_specs_df = pd.DataFrame(specs_list, columns=list(grid.keys()))\n",
    "    \n",
    "    return model_specs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PH_pN6Ht7pbk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_models_list(dataset: str, frequency: str, ensemble_grid: dict, table_width: int):\n",
    "    freq = NBEATSHyperparameters(dataset=dataset, frequency=frequency, ensemble_grid=ensemble_grid)\n",
    "\n",
    "    freq_grid_table = pd.Series({**freq.grid, **freq.ensemble_grid})\n",
    "    freq_table_header  = f'\\n{freq.group.name} '\n",
    "    freq_table_header += 'grid (# of different model configurations = '\n",
    "    freq_table_header += f'{len(_parameter_grid({**freq.grid, **freq.ensemble_grid}))}):\\n'      \n",
    "    print(f'{freq_table_header}{table_width*\"=\"}\\n{freq_grid_table}\\n{table_width*\"=\"}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6iKvr-AJ7pbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_tensorboard(logs_path, model_path):\n",
    "    logs_model_path = f'{logs_path}/{model_path}'\n",
    "#     %load_ext tensorboard\n",
    "#     %tensorboard --logdir $logs_model_path\n",
    "    # os.system('load_ext tensorboard')\n",
    "    # os.system('tensorboard --logdir $logs_model_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PbNVmkM37pbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tourism_evaluation(y_hat_list, models_list, Y_df, S_df, dataset, hparams, num_workers):\n",
    "    _, _, test_loader = create_loaders(Y_df=Y_df, \n",
    "                                       S_df=S_df, \n",
    "                                       dataset=dataset,\n",
    "                                       hparams=hparams, \n",
    "                                       num_workers=num_workers)\n",
    "\n",
    "    mape_list = []\n",
    "\n",
    "    for y_hat in tqdm(y_hat_list, position=0, leave=True):\n",
    "\n",
    "        y_full_batches = np.vstack([batch['Y'].detach().numpy() for batch in test_loader])\n",
    "        sample_mask_full_batches = np.vstack([batch['sample_mask'].detach().numpy() for batch in test_loader])\n",
    "        y_true = y_full_batches * sample_mask_full_batches\n",
    "        y_true = pd.DataFrame(np.reshape(y_true[sample_mask_full_batches ==  1], \n",
    "                                            (y_full_batches.shape[0],-1))).to_numpy()\n",
    "        mape_list.append(100 * np.nanmean(np.nanmean(np.abs(y_true - y_hat) / np.abs(y_true), axis=1), axis=0))\n",
    "\n",
    "    if len(y_hat_list) == 1:\n",
    "        return mape_list[0]\n",
    "    else:\n",
    "        return pd.DataFrame.from_dict({'model': models_list, 'MAPE': mape_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z55V9hMY7pbo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def m4_evaluation(y_hat_list, models_list, frequency):\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for i, y_hat in enumerate(tqdm(y_hat_list, position=0, leave=True)):\n",
    "\n",
    "        results = {}\n",
    "        \n",
    "        m4_evaluation = M4Evaluation.evaluate('data', frequency, y_hat)\n",
    "        results['model'] = models_list[i]\n",
    "        results['SMAPE'] = m4_evaluation['SMAPE'].item()\n",
    "        results['MASE'] = m4_evaluation['MASE'].item()\n",
    "        results['OWA'] = m4_evaluation['OWA'].item()\n",
    "\n",
    "        results_list.append(results)\n",
    "\n",
    "    if len(y_hat_list) == 1:\n",
    "        return results_list[0]\n",
    "    else:\n",
    "        return pd.DataFrame.from_dict(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bhv0mjOg7pbo",
   "metadata": {},
   "source": [
    "# NBEATS Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AAlm1vNO7pbp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def nbeats_instantiate(hparams):\n",
    "\n",
    "    model = NBEATS(n_time_in=int(hparams['n_time_in']),\n",
    "                   n_time_out=int(hparams['n_time_out']),\n",
    "                   n_x=hparams['n_x'],\n",
    "                   n_s=hparams['n_s'],\n",
    "                   n_s_hidden=int(hparams['n_s_hidden']),\n",
    "                   n_x_hidden=int(hparams['n_x_hidden']),\n",
    "                   shared_weights=hparams['shared_weights'],\n",
    "                   initialization=hparams['initialization'],\n",
    "                   activation=hparams['activation'],\n",
    "                   stack_types=hparams['stack_types'],\n",
    "                   n_blocks=hparams['n_blocks'],\n",
    "                   n_layers=hparams['n_layers'],\n",
    "                   n_theta_hidden=hparams['n_theta_hidden'],\n",
    "                   n_harmonics=int(hparams['n_harmonics']),\n",
    "                   n_polynomials=int(hparams['n_polynomials']),\n",
    "                   batch_normalization = hparams['batch_normalization'],\n",
    "                   dropout_prob_theta=hparams['dropout_prob_theta'],\n",
    "                   learning_rate=float(hparams['learning_rate']),\n",
    "                   lr_decay=float(hparams['lr_decay']),\n",
    "                   lr_decay_step_size=float(hparams['lr_decay_step_size']),\n",
    "                   weight_decay=hparams['weight_decay'],\n",
    "                   loss_train=hparams['loss_train'],\n",
    "                   loss_hypar=int(hparams['seasonality']),\n",
    "                   loss_valid=hparams['loss_val'],\n",
    "                   frequency=hparams['frequency'],\n",
    "                   seasonality=int(hparams['seasonality']),\n",
    "                   random_seed=int(hparams['random_seed']))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KZGQToSt7pbq",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NBEATSEnsemble:\n",
    "\n",
    "    def __init__(self, \n",
    "                 dataset: str,\n",
    "                 frequency: str, \n",
    "                 ensemble_grid: dict,\n",
    "                 use_gpus: bool=False, gpus: int=None, auto_select_gpus: bool=False):\n",
    "        \n",
    "        if use_gpus:\n",
    "            assert isinstance(gpus, (int, list, str)), \\\n",
    "                f'if use_gpus == True, gpus must be {int}, {list} or {str}, not {type(gpus)}.'\n",
    "            if (isinstance(gpus, int)):\n",
    "                assert gpus > 0 or gpus == -1, \\\n",
    "                    f'if gpus is of type {int}, it must be either a positive integer or equal to -1.'\n",
    "        else:\n",
    "            assert gpus == None, f'if use_gpus == False, gpus must be {None}, not {type(gpus)}.'\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.frequency = NBEATSHyperparameters(dataset=dataset,\n",
    "                                               frequency=frequency, \n",
    "                                               ensemble_grid=ensemble_grid)\n",
    "        self.gpus = gpus\n",
    "        self.auto_select_gpus = auto_select_gpus\n",
    "\n",
    "    def fit(self,\n",
    "            loader: callable,\n",
    "            val_freq_steps: int,\n",
    "            early_stopping_params: dict,\n",
    "            tensorboard_logs: bool,\n",
    "            logs_path: str,\n",
    "            save_file_path: str,\n",
    "            num_workers: int):\n",
    "\n",
    "        idx_ensemble = 0\n",
    "\n",
    "        if self.dataset == 'M4': Y_df, _, S_df = M4.load(directory='data', group=self.frequency.group.name)\n",
    "        if self.dataset == 'Tourism': Y_df, _, S_df = Tourism.load(directory='data', group=self.frequency.group.name)\n",
    "\n",
    "        freq_grid = _parameter_grid(self.frequency.grid)\n",
    "        forecasts = []\n",
    "\n",
    "        if tensorboard_logs and Path(f'{logs_path}/{self.frequency.group.name}').exists():\n",
    "            shutil.rmtree(f'{logs_path}/{self.frequency.group.name}')\n",
    "            show_tensorboard(logs_path=logs_path, model_path=self.frequency.group.name)\n",
    "\n",
    "        for idx_hparams, row_hparams in freq_grid.iterrows():                             \n",
    "            hparams = row_hparams.to_dict()\n",
    "            train_loader, val_loader, test_loader = create_loaders(Y_df=Y_df, \n",
    "                                                                   S_df=S_df, \n",
    "                                                                   dataset=self.dataset,\n",
    "                                                                   hparams=hparams, \n",
    "                                                                   num_workers=num_workers)\n",
    "\n",
    "            ensemble_grid = _parameter_grid(self.frequency.ensemble_grid)\n",
    "\n",
    "            for idx_ensemble_hparams, row_ensemble_hparams in ensemble_grid.iterrows():\n",
    "                clear_output(wait=True)\n",
    "                idx_ensemble += 1\n",
    "                hparams_ensemble = {**hparams, **row_ensemble_hparams.to_dict()}\n",
    "\n",
    "                model = nbeats_instantiate(hparams_ensemble)\n",
    "                self.print_model_version(hparams_ensemble, idx_ensemble)\n",
    "\n",
    "                if tensorboard_logs: logger = self.create_logger(hparams_ensemble,\n",
    "                                                                 logs_path)\n",
    "                else: logger = False\n",
    "\n",
    "                \n",
    "                early_stop = EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=early_stopping_params['min_delta'],\n",
    "                                        patience=early_stopping_params['patience'],\n",
    "                                        strict=early_stopping_params['strict'],\n",
    "                                        verbose=False,\n",
    "                                        mode='min',\n",
    "                                        check_on_train_epoch_end=False)\n",
    "\n",
    "                trainer = pl.Trainer(max_steps=hparams_ensemble['n_steps'],\n",
    "                                     gradient_clip_val=0,\n",
    "                                     progress_bar_refresh_rate=50, \n",
    "                                     gpus=self.gpus,\n",
    "                                     auto_select_gpus=self.auto_select_gpus, \n",
    "                                     check_val_every_n_epoch=val_freq_steps,\n",
    "                                     checkpoint_callback=tensorboard_logs,\n",
    "                                     callbacks=[early_stop],\n",
    "                                     logger=logger)           \n",
    "                  \n",
    "                # trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
    "                trainer.fit(model, train_dataloader=train_loader, val_dataloaders=test_loader)\n",
    "                \n",
    "                if self.gpus != 1:\n",
    "                    #distributed training\n",
    "                    #predict not supported, only using 1 gpu\n",
    "                    trainer = pl.Trainer(max_steps=hparams_ensemble['n_steps'],\n",
    "                                     gradient_clip_val=0,\n",
    "                                     progress_bar_refresh_rate=50, \n",
    "                                     gpus=1 if torch.cuda.is_available() else None,\n",
    "                                     auto_select_gpus=self.auto_select_gpus, \n",
    "                                     check_val_every_n_epoch=val_freq_steps,\n",
    "                                     checkpoint_callback=tensorboard_logs,\n",
    "                                     logger=logger)    \n",
    "                    \n",
    "                outputs = trainer.predict(model, test_loader)\n",
    "                outputs_df = self.outputs_to_df(outputs, hparams_ensemble)\n",
    "                forecasts.append(outputs_df.copy())\n",
    "\n",
    "                del trainer, model, outputs, outputs_df\n",
    "\n",
    "            del train_loader, val_loader, test_loader\n",
    "\n",
    "        ensembled_forecasts = pd.concat(forecasts).groupby('unique_id').median(0)\n",
    "        ensembled_forecasts.reset_index(inplace=True) \n",
    "        ensembled_forecasts.drop(columns='unique_id', inplace=True)\n",
    "\n",
    "        # Results\n",
    "        time_id = str(time.time()).split(sep='.')[-1]\n",
    "\n",
    "        hparams = freq_grid.iloc[0].to_dict()\n",
    "\n",
    "        results_file_name  = save_file_path\n",
    "        results_file_name += f'/{self.dataset}_{self.frequency.group.name}_{time_id}.pkl'\n",
    "        results = {'dataset': self.frequency.dataset,\n",
    "                   'frequency': self.frequency.frequency,\n",
    "                   'Y_df': Y_df,\n",
    "                   'S_df': S_df,\n",
    "                   'hparams': hparams,\n",
    "                   'forecasts_list': forecasts,\n",
    "                   'ensembled_forecasts': ensembled_forecasts,\n",
    "                   'file_name': results_file_name}\n",
    "\n",
    "        \n",
    "        if self.dataset == 'Tourism':\n",
    "            results['MAPE'] = tourism_evaluation(y_hat_list=[ensembled_forecasts.to_numpy()], \n",
    "                                                 models_list=['ensemble'],\n",
    "                                                 Y_df=Y_df, \n",
    "                                                 S_df=S_df, \n",
    "                                                 dataset=self.dataset, \n",
    "                                                 hparams=hparams, \n",
    "                                                 num_workers=num_workers)\n",
    "\n",
    "        if self.dataset == 'M4':            \n",
    "            m4_results = m4_evaluation(y_hat_list=[ensembled_forecasts.to_numpy()],\n",
    "                                       models_list=['ensemble'],\n",
    "                                       frequency=self.frequency.frequency)\n",
    "\n",
    "            results['SMAPE'] = m4_results['SMAPE']\n",
    "            results['MASE'] = m4_results['MASE']\n",
    "            results['OWA'] = m4_results['OWA']\n",
    "        \n",
    "        with open(results_file_name, 'wb') as f: pickle.dump(results, f)        \n",
    "\n",
    "        return results\n",
    "\n",
    "    def outputs_to_df(self, outputs, hparams):\n",
    "\n",
    "        version  = f'loss-{hparams[\"loss_train\"]}_'\n",
    "        version += f'lbl-{hparams[\"n_time_in\"] // self.frequency.group.horizon}_'\n",
    "        version += f'rs-{hparams[\"random_seed\"]}_'\n",
    "        version += f'lr-{hparams[\"learning_rate\"]}'\n",
    "\n",
    "        outputs_df = torch.vstack([outputs[i][1] \\\n",
    "                                   for i in range(len(outputs))]).detach().cpu().numpy()\n",
    "        outputs_df = pd.DataFrame(outputs_df)\n",
    "        outputs_df.insert(0, 'unique_id', np.arange(outputs_df.shape[0]))\n",
    "        outputs_df.insert(1, 'model', version)\n",
    "\n",
    "        return outputs_df\n",
    "\n",
    "    def create_logger(self, hparams, logs_path):\n",
    "        name = self.frequency.group.name\n",
    "        version  = f'loss-{hparams[\"loss_train\"]}_'\n",
    "        version += f'lbl-{hparams[\"n_time_in\"] // self.frequency.group.horizon}_'\n",
    "        version += f'rs-{hparams[\"random_seed\"]}_'\n",
    "        version += f'lr-{hparams[\"learning_rate\"]}'\n",
    "\n",
    "        logger = TensorBoardLogger(logs_path, name=name, version=version, default_hp_metric=False)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def print_model_version(self, hparams, idx_ensemble):\n",
    "        n_models = len(self.frequency.ensemble_grid['loss_train']) * \\\n",
    "                   len(self.frequency.grid['n_time_in']) * \\\n",
    "                   len(self.frequency.ensemble_grid['random_seed']) * \\\n",
    "                   len(self.frequency.ensemble_grid['learning_rate'])\n",
    "        model_version  = f'\\n{self.frequency.group.name} ({idx_ensemble}/{n_models}) - '\n",
    "        model_version += f'loss: {hparams[\"loss_train\"]}, ' \n",
    "        model_version += f'lookback length: {hparams[\"n_time_in\"] // self.frequency.group.horizon}, '\n",
    "        model_version += f'random_seed: {hparams[\"random_seed\"]} '\n",
    "        model_version += f'learning_rate: {hparams[\"learning_rate\"]}'\n",
    "        print(model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjRKLXBZ7pby",
   "metadata": {},
   "source": [
    "# Ensemble Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T2XPhNGQ7pb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def main(args):\n",
    "    \n",
    "    FREQUENCY = args.frequency\n",
    "    DATASET = args.dataset\n",
    "    LOGS_PATH = Path(f'lightning_logs/{DATASET}')\n",
    "    RESULTS_FILE_PATH = 'nbs/nbeats_results'\n",
    "    NUM_WORKERS = 4\n",
    "    tensorboard_logs = False\n",
    "\n",
    "    ensemble_grid = {'loss_train': ['SMAPE'],\n",
    "#                      'loss_train': ['MAPE', 'SMAPE', 'MASE'],\n",
    "                     'lookbacks': list(range(2, 8)),\n",
    "                     'random_seed': np.arange(args.n_random_seeds).tolist(),\n",
    "                     'learning_rate': np.exp(np.random.uniform(np.log(1e-2), np.log(1e-5), args.n_lr)).tolist(),\n",
    "                     'n_steps': [args.n_iterations]}\n",
    "\n",
    "    print_models_list(dataset=DATASET, frequency=FREQUENCY, ensemble_grid=ensemble_grid, table_width=72)\n",
    "    \n",
    "    ensemble = NBEATSEnsemble(dataset=DATASET,\n",
    "                          frequency=FREQUENCY,\n",
    "                          ensemble_grid=ensemble_grid,\n",
    "                          use_gpus=True if torch.cuda.is_available() else False, \n",
    "                          gpus=-1 if torch.cuda.is_available() else None, \n",
    "                          auto_select_gpus=True if torch.cuda.is_available() else False)\n",
    "\n",
    "    start = time.time()\n",
    "    val_freq_steps = 115\n",
    "\n",
    "    early_stopping_params = {'min_delta': 0.0, 'patience': args.es_patience, 'strict': False}\n",
    "\n",
    "    forecasts = ensemble.fit(loader=TimeSeriesLoader,  \n",
    "                             val_freq_steps=val_freq_steps,\n",
    "                             early_stopping_params=early_stopping_params,\n",
    "                             tensorboard_logs=tensorboard_logs,\n",
    "                             logs_path=LOGS_PATH,   \n",
    "                             save_file_path=RESULTS_FILE_PATH,\n",
    "                             num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f\"Results in file {forecasts['file_name']}\")\n",
    "    print(f'Elapsed time: {time.time() - start}')\n",
    "    \n",
    "    import pickle\n",
    "    with open(forecasts['file_name'], 'rb') as f: ensemble_results = pickle.load(f)\n",
    "\n",
    "    forecasts_by_model = ensemble_results['forecasts_list']\n",
    "\n",
    "    if ensemble_results['dataset'] == 'Tourism':\n",
    "        model_performances = tourism_evaluation(y_hat_list=[fcst.iloc[:, 2:].to_numpy() for fcst in forecasts_by_model], \n",
    "                                                models_list=[fcst.model[0] for fcst in forecasts_by_model],\n",
    "                                                Y_df=ensemble_results['Y_df'], \n",
    "                                                S_df=ensemble_results['S_df'], \n",
    "                                                dataset=ensemble_results['dataset'], \n",
    "                                                hparams=ensemble_results['hparams'], \n",
    "                                                num_workers=NUM_WORKERS)\n",
    "\n",
    "    if ensemble_results['dataset'] == 'M4':\n",
    "        model_performances = m4_evaluation(y_hat_list=[fcst.iloc[:, 2:].to_numpy() for fcst in forecasts_by_model],\n",
    "                                           models_list=[fcst.model[0] for fcst in forecasts_by_model],\n",
    "                                           frequency=FREQUENCY)\n",
    "        \n",
    "    print(model_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GsbHOZwg7pb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_args():\n",
    "    desc = \"NBEATS LR and Random Seed tuning\"\n",
    "    parser = argparse.ArgumentParser(description=desc)\n",
    "    parser.add_argument('--dataset', required=True, type=str, help='Dataset, either Tourism or M4')\n",
    "    parser.add_argument('--frequency', required=True, type=str, help='Dataset time frequency')\n",
    "    parser.add_argument('--n_random_seeds', required=True, type=int, help='Number of random seeds to test')\n",
    "    parser.add_argument('--n_lr', required=True, type=int, help='Number of learning rates to test')\n",
    "    parser.add_argument('--n_iterations', required=True, type=int, help='Number of iterations')\n",
    "    parser.add_argument('--es_patience', required=True, type=int, help='Early stopping patience')\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7yyv-wF67pb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "#     args = pd.Series({'dataset': 'Tourism',\n",
    "#                       'frequency': 'Yearly',\n",
    "#                       'n_random_seeds': 1,\n",
    "#                       'n_lr': 1,\n",
    "#                       'n_iterations': 15,\n",
    "#                       'es_patience': 10})\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KpGyj05I7pb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m nixtlats.models.nbeats.tuning --dataset M4 --frequency Yearly --n_random_seeds 1 --n_lr 1 --n_iterations 100 --es_patience 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_nixtla)",
   "language": "python",
   "name": "conda_nixtla"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
