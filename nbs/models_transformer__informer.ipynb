{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.transformer.informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "\n",
    "from neuralforecast.models.components.transformer import Decoder, DecoderLayer, Encoder, EncoderLayer, ConvLayer\n",
    "from neuralforecast.models.components.selfattention import (\n",
    "    TriangularCausalMask, ProbMask, \n",
    "    FullAttention, ProbAttention, AttentionLayer\n",
    ")\n",
    "from neuralforecast.models.components.embed import DataEmbedding\n",
    "from neuralforecast.losses.utils import LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _Informer(nn.Module):\n",
    "    \"\"\"\n",
    "    Informer with Propspare attention in O(LlogL) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, pred_len, output_attention,\n",
    "                 enc_in, dec_in, d_model, c_out, embed, freq, dropout,\n",
    "                 factor, n_heads, d_ff, activation, e_layers,\n",
    "                 d_layers, distil):\n",
    "        super(_Informer, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq,\n",
    "                                           dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq,\n",
    "                                           dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        ProbAttention(False, factor, attention_dropout=dropout,\n",
    "                                      output_attention=output_attention),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            [\n",
    "                ConvLayer(\n",
    "                    d_model\n",
    "                ) for l in range(e_layers - 1)\n",
    "            ] if distil else None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        ProbAttention(True, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        ProbAttention(False, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model),\n",
    "            projection=nn.Linear(d_model, c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Informer(pl.LightningModule):\n",
    "    def __init__(self, seq_len, \n",
    "                 label_len, pred_len, output_attention,\n",
    "                 enc_in, dec_in, d_model, c_out, embed, freq, dropout,\n",
    "                 factor, n_heads, d_ff, activation, e_layers, d_layers, distil,\n",
    "                 loss_train, loss_valid, loss_hypar, learning_rate,\n",
    "                 lr_decay, weight_decay, lr_decay_step_size,\n",
    "                 random_seed):\n",
    "        super(Informer, self).__init__()\n",
    "\n",
    "        #------------------------ Model Attributes ------------------------#\n",
    "        # Architecture parameters\n",
    "        self.seq_len = seq_len \n",
    "        self.label_len = label_len \n",
    "        self.pred_len = pred_len \n",
    "        self.output_attention = output_attention\n",
    "        self.enc_in = enc_in \n",
    "        self.dec_in = dec_in \n",
    "        self.d_model = d_model \n",
    "        self.c_out = c_out \n",
    "        self.embed = embed\n",
    "        self.freq = freq \n",
    "        self.dropout = dropout\n",
    "        self.factor = factor \n",
    "        self.n_heads = n_heads \n",
    "        self.d_ff = d_ff \n",
    "        self.activation = activation \n",
    "        self.e_layers = e_layers\n",
    "        self.d_layers = d_layers\n",
    "        self.distil = distil\n",
    "        \n",
    "        # Loss functions\n",
    "        self.loss_train = loss_train\n",
    "        self.loss_hypar = loss_hypar\n",
    "        self.loss_valid = loss_valid\n",
    "        self.loss_fn_train = LossFunction(loss_train, \n",
    "                                          seasonality=self.loss_hypar)\n",
    "        self.loss_fn_valid = LossFunction(loss_valid,\n",
    "                                          seasonality=self.loss_hypar)\n",
    "        \n",
    "        # Regularization and optimization parameters      \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay = lr_decay\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_decay_step_size = lr_decay_step_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.model = _Informer(pred_len, output_attention,\n",
    "                               enc_in, dec_in, d_model, c_out, \n",
    "                               embed, freq, dropout,\n",
    "                               factor, n_heads, d_ff, \n",
    "                               activation, e_layers,\n",
    "                               d_layers, distil)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Autoformer needs batch of shape (batch_size, time, series) for y\n",
    "        and (batch_size, time, exogenous) for x\n",
    "        and doesnt need X for each time series.\n",
    "        USE DataLoader from pytorch instead of TimeSeriesLoader.\n",
    "        \"\"\"\n",
    "        Y = batch['Y'].permute(0, 2, 1)\n",
    "        X = batch['X'][:, 0, :, :].permute(0, 2, 1)\n",
    "        sample_mask = batch['sample_mask'].permute(0, 2, 1)\n",
    "        available_mask = batch['available_mask']\n",
    "        \n",
    "        s_begin = 0\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        batch_x = Y[:, s_begin:s_end, :]\n",
    "        batch_y = Y[:, r_begin:r_end, :]\n",
    "        batch_x_mark = X[:, s_begin:s_end, :]\n",
    "        batch_y_mark = X[:, r_begin:r_end, :]\n",
    "        outsample_mask = sample_mask[:, r_begin:r_end, :]\n",
    "        \n",
    "        dec_inp = torch.zeros_like(batch_y[:, -self.pred_len:, :])\n",
    "        dec_inp = torch.cat([batch_y[:, :self.label_len, :], dec_inp], dim=1)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "        else:\n",
    "            forecast = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            \n",
    "        batch_y = batch_y[:, -self.pred_len:, :]\n",
    "        outsample_mask = outsample_mask[:, -self.pred_len:, :]\n",
    "\n",
    "        return batch_y, forecast, outsample_mask\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        outsample_y, forecast, outsample_mask = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_train(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=batch['Y'].permute(0, 2, 1))\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        \n",
    "        outsample_y, forecast, outsample_mask = self(batch)\n",
    "\n",
    "        loss = self.loss_fn_valid(y=outsample_y,\n",
    "                                  y_hat=forecast,\n",
    "                                  mask=outsample_mask,\n",
    "                                  y_insample=batch['Y'].permute(0, 2, 1))\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        np.random.seed(self.random_seed)\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.learning_rate, \n",
    "                               weight_decay=self.weight_decay)\n",
    "        \n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                 step_size=self.lr_decay_step_size, \n",
    "                                                 gamma=self.lr_decay)\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informer Usage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.data.datasets.long_horizon import LongHorizon\n",
    "\n",
    "Y_df, X_df, S_df = LongHorizon.load(directory='./data', group='ETTm2')\n",
    "Y_df = Y_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>41.130001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>39.622002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:30:00</td>\n",
       "      <td>38.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:45:00</td>\n",
       "      <td>35.518002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>37.528000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                  ds          y\n",
       "0      HUFL 2016-07-01 00:00:00  41.130001\n",
       "1      HUFL 2016-07-01 00:15:00  39.622002\n",
       "2      HUFL 2016-07-01 00:30:00  38.868000\n",
       "3      HUFL 2016-07-01 00:45:00  35.518002\n",
       "4      HUFL 2016-07-01 01:00:00  37.528000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>HourOfDay</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DayOfMonth</th>\n",
       "      <th>DayOfYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:15:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:30:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 00:45:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUFL</td>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>-0.456522</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.00137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id                  ds  HourOfDay  DayOfWeek  DayOfMonth  DayOfYear\n",
       "0      HUFL 2016-07-01 00:00:00  -0.500000   0.166667        -0.5   -0.00137\n",
       "1      HUFL 2016-07-01 00:15:00  -0.500000   0.166667        -0.5   -0.00137\n",
       "2      HUFL 2016-07-01 00:30:00  -0.500000   0.166667        -0.5   -0.00137\n",
       "3      HUFL 2016-07-01 00:45:00  -0.500000   0.166667        -0.5   -0.00137\n",
       "4      HUFL 2016-07-01 01:00:00  -0.456522   0.166667        -0.5   -0.00137"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = X_df.drop(columns=['unique_id', 'ds']).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model and Data Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- s_begin = index\n",
    "- s_end = index + self.seq_len\n",
    "- r_begin = index + self.seq_len - self.label_len\n",
    "- r_end = index + self.seq_len + self.pred_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "mc_model = {}\n",
    "\n",
    "mc_model['seq_len'] = 96\n",
    "mc_model['label_len'] = 48\n",
    "mc_model['pred_len'] = 24\n",
    "mc_model['output_attention'] = False\n",
    "mc_model['enc_in'] = 7\n",
    "mc_model['dec_in'] = 7\n",
    "mc_model['d_model'] = 512\n",
    "mc_model['c_out'] = 7\n",
    "mc_model['embed'] = 'timeF'\n",
    "mc_model['freq'] = 'h'\n",
    "mc_model['dropout'] = 0.05\n",
    "mc_model['factor'] = 1\n",
    "mc_model['n_heads'] = 8\n",
    "mc_model['d_ff'] = 2_048\n",
    "mc_model['activation'] = 'gelu'\n",
    "mc_model['e_layers'] = 2 \n",
    "mc_model['d_layers'] = 1\n",
    "mc_model['distil'] = None\n",
    "mc_model['loss_train'] = 'MAE'\n",
    "mc_model['loss_hypar'] = 0.5\n",
    "mc_model['loss_valid'] = 'MAE'\n",
    "mc_model['learning_rate'] = 0.001\n",
    "mc_model['lr_decay'] = 0.5\n",
    "mc_model['weight_decay'] = 0.\n",
    "mc_model['lr_decay_step_size'] = 2\n",
    "mc_model['random_seed'] = 1\n",
    "\n",
    "# Dataset parameters\n",
    "mc_data = {}\n",
    "mc_data['mode'] = 'iterate_windows'\n",
    "mc_data['n_time_in'] = mc_model['seq_len']\n",
    "mc_data['n_time_out'] = mc_model['pred_len']\n",
    "mc_data['batch_size'] = 32\n",
    "mc_data['normalizer_y'] = None\n",
    "mc_data['normalizer_x'] = None\n",
    "mc_data['max_epochs'] = 1\n",
    "mc_data['max_steps'] = 2\n",
    "mc_data['early_stop_patience'] = 20\n",
    "\n",
    "len_val = 11_520\n",
    "len_test = 11_520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Loaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.data.tsdataset import IterateWindowsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                       ds                    \n",
      "                                      min                 max\n",
      "unique_id sample_mask                                        \n",
      "HUFL      0           2017-10-29 20:00:00 2018-06-26 19:45:00\n",
      "          1           2016-07-01 00:00:00 2017-10-29 19:45:00\n",
      "HULL      0           2017-10-29 20:00:00 2018-06-26 19:45:00\n",
      "          1           2016-07-01 00:00:00 2017-10-29 19:45:00\n",
      "MUFL      0           2017-10-29 20:00:00 2018-06-26 19:45:00\n",
      "          1           2016-07-01 00:00:00 2017-10-29 19:45:00\n",
      "MULL      0           2017-10-29 20:00:00 2018-06-26 19:45:00\n",
      "          1           2016-07-01 00:00:00 2017-10-29 19:45:00\n",
      "LUFL      0           2017-10-29 20:00:00 2018-06-26 19:45:00\n",
      "          1           2016-07-01 00:00:00 2017-10-29 19:45:00\n",
      "LULL      0           2017-10-29 20:00:00 2018-06-26 19:45:00\n",
      "          1           2016-07-01 00:00:00 2017-10-29 19:45:00\n",
      "OT        0           2017-10-29 20:00:00 2018-06-26 19:45:00\n",
      "          1           2016-07-01 00:00:00 2017-10-29 19:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t487760 time stamps \n",
      "Available percentage=100.0, \t487760 time stamps \n",
      "Insample  percentage=66.93, \t326480 time stamps \n",
      "Outsample percentage=33.07, \t161280 time stamps \n",
      "\n",
      "/Users/fedex/projects/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                       ds                    \n",
      "                                      min                 max\n",
      "unique_id sample_mask                                        \n",
      "HUFL      0           2016-07-01 00:00:00 2018-06-26 19:45:00\n",
      "          1           2017-10-29 20:00:00 2018-02-26 19:45:00\n",
      "HULL      0           2016-07-01 00:00:00 2018-06-26 19:45:00\n",
      "          1           2017-10-29 20:00:00 2018-02-26 19:45:00\n",
      "MUFL      0           2016-07-01 00:00:00 2018-06-26 19:45:00\n",
      "          1           2017-10-29 20:00:00 2018-02-26 19:45:00\n",
      "MULL      0           2016-07-01 00:00:00 2018-06-26 19:45:00\n",
      "          1           2017-10-29 20:00:00 2018-02-26 19:45:00\n",
      "LUFL      0           2016-07-01 00:00:00 2018-06-26 19:45:00\n",
      "          1           2017-10-29 20:00:00 2018-02-26 19:45:00\n",
      "LULL      0           2016-07-01 00:00:00 2018-06-26 19:45:00\n",
      "          1           2017-10-29 20:00:00 2018-02-26 19:45:00\n",
      "OT        0           2016-07-01 00:00:00 2018-06-26 19:45:00\n",
      "          1           2017-10-29 20:00:00 2018-02-26 19:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t487760 time stamps \n",
      "Available percentage=100.0, \t487760 time stamps \n",
      "Insample  percentage=16.53, \t80640 time stamps \n",
      "Outsample percentage=83.47, \t407120 time stamps \n",
      "\n",
      "/Users/fedex/projects/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                                       ds                    \n",
      "                                      min                 max\n",
      "unique_id sample_mask                                        \n",
      "HUFL      0           2016-07-01 00:00:00 2018-02-26 19:45:00\n",
      "          1           2018-02-26 20:00:00 2018-06-26 19:45:00\n",
      "HULL      0           2016-07-01 00:00:00 2018-02-26 19:45:00\n",
      "          1           2018-02-26 20:00:00 2018-06-26 19:45:00\n",
      "MUFL      0           2016-07-01 00:00:00 2018-02-26 19:45:00\n",
      "          1           2018-02-26 20:00:00 2018-06-26 19:45:00\n",
      "MULL      0           2016-07-01 00:00:00 2018-02-26 19:45:00\n",
      "          1           2018-02-26 20:00:00 2018-06-26 19:45:00\n",
      "LUFL      0           2016-07-01 00:00:00 2018-02-26 19:45:00\n",
      "          1           2018-02-26 20:00:00 2018-06-26 19:45:00\n",
      "LULL      0           2016-07-01 00:00:00 2018-02-26 19:45:00\n",
      "          1           2018-02-26 20:00:00 2018-06-26 19:45:00\n",
      "OT        0           2016-07-01 00:00:00 2018-02-26 19:45:00\n",
      "          1           2018-02-26 20:00:00 2018-06-26 19:45:00\n",
      "INFO:root:\n",
      "Total data \t\t\t487760 time stamps \n",
      "Available percentage=100.0, \t487760 time stamps \n",
      "Insample  percentage=16.53, \t80640 time stamps \n",
      "Outsample percentage=83.47, \t407120 time stamps \n",
      "\n",
      "/Users/fedex/projects/nixtlats/nixtlats/data/tsdataset.py:208: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X.drop(['unique_id', 'ds'], 1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from neuralforecast.experiments.utils import create_datasets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc_data,\n",
    "                                                                     S_df=None, \n",
    "                                                                     Y_df=Y_df, X_df=X_df,\n",
    "                                                                     f_cols=f_cols,\n",
    "                                                                     ds_in_val=len_val,\n",
    "                                                                     ds_in_test=len_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=int(mc_data['batch_size']),\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=int(mc_data['batch_size']),\n",
    "                        shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=int(mc_data['batch_size']),\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Informer(**mc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fedex/opt/miniconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/fedex/opt/miniconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:122: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | _Informer | 10.5 M\n",
      "------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "42.160    Total estimated model params size (MB)\n",
      "/Users/fedex/opt/miniconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ac7c9df35e429eb9f898b8884ebee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"train_loss\", \n",
    "                               min_delta=1e-4, \n",
    "                               patience=mc_data['early_stop_patience'],\n",
    "                               verbose=False,\n",
    "                               mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=mc_data['max_epochs'], \n",
    "                     max_steps=mc_data['max_steps'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     progress_bar_refresh_rate=10, \n",
    "                     log_every_n_steps=500, \n",
    "                     check_val_every_n_epoch=1,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = trainer.predict(model, val_loader)\n",
    "\n",
    "#print(\"outputs[0][0].shape\", outputs[0][0].shape)\n",
    "#print(\"outputs[0][1].shape\", outputs[0][1].shape)\n",
    "#print(\"outputs[0][2].shape\", outputs[0][2].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
